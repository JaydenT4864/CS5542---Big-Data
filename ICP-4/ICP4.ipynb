{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZrrtyA-Ihi_"
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.constraints import maxnorm\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn36mR6gNhiq"
      },
      "source": [
        "In this example, we will be using the famous CIFAR-10 dataset. CIFAR-10 is a large image dataset containing over 60,000 images representing 10 different classes of objects like cats, planes, and cars.\n",
        "\n",
        "The images are full-color RGB, but they are fairly small, only 32 x 32. One great thing about the CIFAR-10 dataset is that it comes prepackaged with Keras, so it is very easy to load up the dataset and the images need very little preprocessing.\n",
        "\n",
        "We're going to be using a random seed here so that the results achieved in this ICP can be replicated by you,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agVj8xIAGu4c"
      },
      "source": [
        "# Set random seed for purposes of reproducibility\n",
        "seed = 21"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smw-A_L8OXRZ"
      },
      "source": [
        "Now let's load in the dataset. We can do so simply by specifying which variables we want to load the data into, and then using the load_data() function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1G2V0ldG-gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab48444b-7738-4dc4-d22b-75952083c166"
      },
      "source": [
        "# loading in the data\n",
        "     \n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOqt5wTAHKyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44f0c6d-927a-4af4-dc44-f4871eb0f8ad"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsjXz3GjhU88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62514e7a-2b20-4a47-f143-7a4c8c74d1a4"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[158 112  49]\n",
            "   [159 111  47]\n",
            "   [165 116  51]\n",
            "   ...\n",
            "   [137  95  36]\n",
            "   [126  91  36]\n",
            "   [116  85  33]]\n",
            "\n",
            "  [[152 112  51]\n",
            "   [151 110  40]\n",
            "   [159 114  45]\n",
            "   ...\n",
            "   [136  95  31]\n",
            "   [125  91  32]\n",
            "   [119  88  34]]\n",
            "\n",
            "  [[151 110  47]\n",
            "   [151 109  33]\n",
            "   [158 111  36]\n",
            "   ...\n",
            "   [139  98  34]\n",
            "   [130  95  34]\n",
            "   [120  89  33]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 68 124 177]\n",
            "   [ 42 100 148]\n",
            "   [ 31  88 137]\n",
            "   ...\n",
            "   [ 38  97 146]\n",
            "   [ 13  64 108]\n",
            "   [ 40  85 127]]\n",
            "\n",
            "  [[ 61 116 168]\n",
            "   [ 49 102 148]\n",
            "   [ 35  85 132]\n",
            "   ...\n",
            "   [ 26  82 130]\n",
            "   [ 29  82 126]\n",
            "   [ 20  64 107]]\n",
            "\n",
            "  [[ 54 107 160]\n",
            "   [ 56 105 149]\n",
            "   [ 45  89 132]\n",
            "   ...\n",
            "   [ 24  77 124]\n",
            "   [ 34  84 129]\n",
            "   [ 21  67 110]]]\n",
            "\n",
            "\n",
            " [[[235 235 235]\n",
            "   [231 231 231]\n",
            "   [232 232 232]\n",
            "   ...\n",
            "   [233 233 233]\n",
            "   [233 233 233]\n",
            "   [232 232 232]]\n",
            "\n",
            "  [[238 238 238]\n",
            "   [235 235 235]\n",
            "   [235 235 235]\n",
            "   ...\n",
            "   [236 236 236]\n",
            "   [236 236 236]\n",
            "   [235 235 235]]\n",
            "\n",
            "  [[237 237 237]\n",
            "   [234 234 234]\n",
            "   [234 234 234]\n",
            "   ...\n",
            "   [235 235 235]\n",
            "   [235 235 235]\n",
            "   [234 234 234]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 87  99  89]\n",
            "   [ 43  51  37]\n",
            "   [ 19  23  11]\n",
            "   ...\n",
            "   [169 184 179]\n",
            "   [182 197 193]\n",
            "   [188 202 201]]\n",
            "\n",
            "  [[ 82  96  82]\n",
            "   [ 46  57  36]\n",
            "   [ 36  44  22]\n",
            "   ...\n",
            "   [174 189 183]\n",
            "   [185 200 196]\n",
            "   [187 202 200]]\n",
            "\n",
            "  [[ 85 101  83]\n",
            "   [ 62  75  48]\n",
            "   [ 58  67  38]\n",
            "   ...\n",
            "   [168 183 178]\n",
            "   [180 195 191]\n",
            "   [186 200 199]]]\n",
            "\n",
            "\n",
            " [[[158 190 222]\n",
            "   [158 187 218]\n",
            "   [139 166 194]\n",
            "   ...\n",
            "   [228 231 234]\n",
            "   [237 239 243]\n",
            "   [238 241 246]]\n",
            "\n",
            "  [[170 200 229]\n",
            "   [172 199 226]\n",
            "   [151 176 201]\n",
            "   ...\n",
            "   [232 232 236]\n",
            "   [246 246 250]\n",
            "   [246 247 251]]\n",
            "\n",
            "  [[174 201 225]\n",
            "   [176 200 222]\n",
            "   [157 179 199]\n",
            "   ...\n",
            "   [230 229 232]\n",
            "   [250 249 251]\n",
            "   [245 244 247]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 31  40  45]\n",
            "   [ 30  39  44]\n",
            "   [ 26  35  40]\n",
            "   ...\n",
            "   [ 37  40  46]\n",
            "   [  9  13  14]\n",
            "   [  4   7   5]]\n",
            "\n",
            "  [[ 23  34  39]\n",
            "   [ 27  38  43]\n",
            "   [ 25  36  41]\n",
            "   ...\n",
            "   [ 19  20  24]\n",
            "   [  4   6   3]\n",
            "   [  5   7   3]]\n",
            "\n",
            "  [[ 28  41  47]\n",
            "   [ 30  43  50]\n",
            "   [ 32  45  52]\n",
            "   ...\n",
            "   [  5   6   8]\n",
            "   [  4   5   3]\n",
            "   [  7   8   7]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 20  15  12]\n",
            "   [ 19  14  11]\n",
            "   [ 15  14  11]\n",
            "   ...\n",
            "   [ 10   9   7]\n",
            "   [ 12  11   9]\n",
            "   [ 13  12  10]]\n",
            "\n",
            "  [[ 21  16  13]\n",
            "   [ 20  16  13]\n",
            "   [ 18  17  12]\n",
            "   ...\n",
            "   [ 10   9   7]\n",
            "   [ 10   9   7]\n",
            "   [ 12  11   9]]\n",
            "\n",
            "  [[ 21  16  13]\n",
            "   [ 21  17  12]\n",
            "   [ 20  18  11]\n",
            "   ...\n",
            "   [ 12  11   9]\n",
            "   [ 12  11   9]\n",
            "   [ 13  12  10]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 33  25  13]\n",
            "   [ 34  26  15]\n",
            "   [ 34  26  15]\n",
            "   ...\n",
            "   [ 28  25  52]\n",
            "   [ 29  25  58]\n",
            "   [ 23  20  42]]\n",
            "\n",
            "  [[ 33  25  14]\n",
            "   [ 34  26  15]\n",
            "   [ 34  26  15]\n",
            "   ...\n",
            "   [ 27  24  52]\n",
            "   [ 27  24  56]\n",
            "   [ 25  22  47]]\n",
            "\n",
            "  [[ 31  23  12]\n",
            "   [ 32  24  13]\n",
            "   [ 33  25  14]\n",
            "   ...\n",
            "   [ 24  23  50]\n",
            "   [ 26  23  53]\n",
            "   [ 25  20  47]]]\n",
            "\n",
            "\n",
            " [[[ 25  40  12]\n",
            "   [ 15  36   3]\n",
            "   [ 23  41  18]\n",
            "   ...\n",
            "   [ 61  82  78]\n",
            "   [ 92 113 112]\n",
            "   [ 75  89  92]]\n",
            "\n",
            "  [[ 12  25   6]\n",
            "   [ 20  37   7]\n",
            "   [ 24  36  15]\n",
            "   ...\n",
            "   [115 134 138]\n",
            "   [149 168 177]\n",
            "   [104 117 131]]\n",
            "\n",
            "  [[ 12  25  11]\n",
            "   [ 15  29   6]\n",
            "   [ 34  40  24]\n",
            "   ...\n",
            "   [154 172 182]\n",
            "   [157 175 192]\n",
            "   [116 129 151]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[100 129  81]\n",
            "   [103 132  84]\n",
            "   [104 134  86]\n",
            "   ...\n",
            "   [ 97 128  84]\n",
            "   [ 98 126  84]\n",
            "   [ 91 121  79]]\n",
            "\n",
            "  [[103 132  83]\n",
            "   [104 131  83]\n",
            "   [107 135  87]\n",
            "   ...\n",
            "   [101 132  87]\n",
            "   [ 99 127  84]\n",
            "   [ 92 121  79]]\n",
            "\n",
            "  [[ 95 126  78]\n",
            "   [ 95 123  76]\n",
            "   [101 128  81]\n",
            "   ...\n",
            "   [ 93 124  80]\n",
            "   [ 95 123  81]\n",
            "   [ 92 120  80]]]\n",
            "\n",
            "\n",
            " [[[ 73  78  75]\n",
            "   [ 98 103 113]\n",
            "   [ 99 106 114]\n",
            "   ...\n",
            "   [135 150 152]\n",
            "   [135 149 154]\n",
            "   [203 215 223]]\n",
            "\n",
            "  [[ 69  73  70]\n",
            "   [ 84  89  97]\n",
            "   [ 68  75  81]\n",
            "   ...\n",
            "   [ 85  95  89]\n",
            "   [ 71  82  80]\n",
            "   [120 133 135]]\n",
            "\n",
            "  [[ 69  73  70]\n",
            "   [ 90  95 100]\n",
            "   [ 62  71  74]\n",
            "   ...\n",
            "   [ 74  81  70]\n",
            "   [ 53  62  54]\n",
            "   [ 62  74  69]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[123 128  96]\n",
            "   [132 132 102]\n",
            "   [129 128 100]\n",
            "   ...\n",
            "   [108 107  88]\n",
            "   [ 62  60  55]\n",
            "   [ 27  27  28]]\n",
            "\n",
            "  [[115 121  91]\n",
            "   [123 124  95]\n",
            "   [129 126  99]\n",
            "   ...\n",
            "   [115 116  94]\n",
            "   [ 66  65  59]\n",
            "   [ 27  27  27]]\n",
            "\n",
            "  [[116 120  90]\n",
            "   [121 122  94]\n",
            "   [129 128 101]\n",
            "   ...\n",
            "   [116 115  94]\n",
            "   [ 68  65  58]\n",
            "   [ 27  26  26]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8CXWy03Hdnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebfbb2b4-0ad0-41d7-8a25-a4ba92991176"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 1)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdQOTwqWHmVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fd7f69-e973-40ee-e31b-21de7a3e5dfe"
      },
      "source": [
        "print (y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3]\n",
            " [8]\n",
            " [8]\n",
            " ...\n",
            " [5]\n",
            " [1]\n",
            " [7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8fwz7HPDM4"
      },
      "source": [
        "In most cases you will need to do some preprocessing of your data to get it ready for use, but since we are using a prepackaged dataset, very little preprocessing needs to be done. One thing we want to do is normalize the input data.\n",
        "\n",
        "If the values of the input data are in too wide a range it can negatively impact how the network performs. In this case, the input values are the pixels in the image, which have a value between 0 to 255.\n",
        "\n",
        "So in order to normalize the data we can simply divide the image values by 255. To do this we first need to make the data a float type, since they are currently integers. We can do this by using the astype() Numpy command and then declaring what data type we want:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu-SWHIoHd0M"
      },
      "source": [
        " # normalize the inputs from 0-255 to between 0 and 1 by dividing by 255   \n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9E19w_6iMQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d37aa7-f4b7-443f-fc2b-f9bc1479043d"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[0.23137255 0.24313726 0.24705882]\n",
            "   [0.16862746 0.18039216 0.1764706 ]\n",
            "   [0.19607843 0.1882353  0.16862746]\n",
            "   ...\n",
            "   [0.61960787 0.5176471  0.42352942]\n",
            "   [0.59607846 0.49019608 0.4       ]\n",
            "   [0.5803922  0.4862745  0.40392157]]\n",
            "\n",
            "  [[0.0627451  0.07843138 0.07843138]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.07058824 0.03137255 0.        ]\n",
            "   ...\n",
            "   [0.48235294 0.34509805 0.21568628]\n",
            "   [0.46666667 0.3254902  0.19607843]\n",
            "   [0.47843137 0.34117648 0.22352941]]\n",
            "\n",
            "  [[0.09803922 0.09411765 0.08235294]\n",
            "   [0.0627451  0.02745098 0.        ]\n",
            "   [0.19215687 0.10588235 0.03137255]\n",
            "   ...\n",
            "   [0.4627451  0.32941177 0.19607843]\n",
            "   [0.47058824 0.32941177 0.19607843]\n",
            "   [0.42745098 0.28627452 0.16470589]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.8156863  0.6666667  0.3764706 ]\n",
            "   [0.7882353  0.6        0.13333334]\n",
            "   [0.7764706  0.6313726  0.10196079]\n",
            "   ...\n",
            "   [0.627451   0.52156866 0.27450982]\n",
            "   [0.21960784 0.12156863 0.02745098]\n",
            "   [0.20784314 0.13333334 0.07843138]]\n",
            "\n",
            "  [[0.7058824  0.54509807 0.3764706 ]\n",
            "   [0.6784314  0.48235294 0.16470589]\n",
            "   [0.7294118  0.5647059  0.11764706]\n",
            "   ...\n",
            "   [0.72156864 0.5803922  0.36862746]\n",
            "   [0.38039216 0.24313726 0.13333334]\n",
            "   [0.3254902  0.20784314 0.13333334]]\n",
            "\n",
            "  [[0.69411767 0.5647059  0.45490196]\n",
            "   [0.65882355 0.5058824  0.36862746]\n",
            "   [0.7019608  0.5568628  0.34117648]\n",
            "   ...\n",
            "   [0.84705883 0.72156864 0.54901963]\n",
            "   [0.5921569  0.4627451  0.32941177]\n",
            "   [0.48235294 0.36078432 0.28235295]]]\n",
            "\n",
            "\n",
            " [[[0.6039216  0.69411767 0.73333335]\n",
            "   [0.49411765 0.5372549  0.53333336]\n",
            "   [0.4117647  0.40784314 0.37254903]\n",
            "   ...\n",
            "   [0.35686275 0.37254903 0.2784314 ]\n",
            "   [0.34117648 0.3529412  0.2784314 ]\n",
            "   [0.30980393 0.31764707 0.27450982]]\n",
            "\n",
            "  [[0.54901963 0.627451   0.6627451 ]\n",
            "   [0.5686275  0.6        0.6039216 ]\n",
            "   [0.49019608 0.49019608 0.4627451 ]\n",
            "   ...\n",
            "   [0.3764706  0.3882353  0.30588236]\n",
            "   [0.3019608  0.3137255  0.24313726]\n",
            "   [0.2784314  0.28627452 0.23921569]]\n",
            "\n",
            "  [[0.54901963 0.60784316 0.6431373 ]\n",
            "   [0.54509807 0.57254905 0.58431375]\n",
            "   [0.4509804  0.4509804  0.4392157 ]\n",
            "   ...\n",
            "   [0.30980393 0.32156864 0.2509804 ]\n",
            "   [0.26666668 0.27450982 0.21568628]\n",
            "   [0.2627451  0.27058825 0.21568628]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.6862745  0.654902   0.6509804 ]\n",
            "   [0.6117647  0.6039216  0.627451  ]\n",
            "   [0.6039216  0.627451   0.6666667 ]\n",
            "   ...\n",
            "   [0.16470589 0.13333334 0.14117648]\n",
            "   [0.23921569 0.20784314 0.22352941]\n",
            "   [0.3647059  0.3254902  0.35686275]]\n",
            "\n",
            "  [[0.64705884 0.6039216  0.5019608 ]\n",
            "   [0.6117647  0.59607846 0.50980395]\n",
            "   [0.62352943 0.6313726  0.5568628 ]\n",
            "   ...\n",
            "   [0.40392157 0.3647059  0.3764706 ]\n",
            "   [0.48235294 0.44705883 0.47058824]\n",
            "   [0.5137255  0.4745098  0.5137255 ]]\n",
            "\n",
            "  [[0.6392157  0.5803922  0.47058824]\n",
            "   [0.61960787 0.5803922  0.47843137]\n",
            "   [0.6392157  0.6117647  0.52156866]\n",
            "   ...\n",
            "   [0.56078434 0.52156866 0.54509807]\n",
            "   [0.56078434 0.5254902  0.5568628 ]\n",
            "   [0.56078434 0.52156866 0.5647059 ]]]\n",
            "\n",
            "\n",
            " [[[1.         1.         1.        ]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   ...\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [0.99607843 0.99607843 0.99607843]\n",
            "   [0.99607843 0.99607843 0.99607843]\n",
            "   ...\n",
            "   [0.99607843 0.99607843 0.99607843]\n",
            "   [0.99607843 0.99607843 0.99607843]\n",
            "   [0.99607843 0.99607843 0.99607843]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.44313726 0.47058824 0.4392157 ]\n",
            "   [0.43529412 0.4627451  0.43529412]\n",
            "   [0.4117647  0.4392157  0.41568628]\n",
            "   ...\n",
            "   [0.28235295 0.31764707 0.3137255 ]\n",
            "   [0.28235295 0.3137255  0.30980393]\n",
            "   [0.28235295 0.3137255  0.30980393]]\n",
            "\n",
            "  [[0.43529412 0.4627451  0.43137255]\n",
            "   [0.40784314 0.43529412 0.40784314]\n",
            "   [0.3882353  0.41568628 0.38431373]\n",
            "   ...\n",
            "   [0.26666668 0.29411766 0.28627452]\n",
            "   [0.27450982 0.29803923 0.29411766]\n",
            "   [0.30588236 0.32941177 0.32156864]]\n",
            "\n",
            "  [[0.41568628 0.44313726 0.4117647 ]\n",
            "   [0.3882353  0.41568628 0.38431373]\n",
            "   [0.37254903 0.4        0.36862746]\n",
            "   ...\n",
            "   [0.30588236 0.33333334 0.3254902 ]\n",
            "   [0.30980393 0.33333334 0.3254902 ]\n",
            "   [0.3137255  0.3372549  0.32941177]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.13725491 0.69803923 0.92156863]\n",
            "   [0.15686275 0.6901961  0.9372549 ]\n",
            "   [0.16470589 0.6901961  0.94509804]\n",
            "   ...\n",
            "   [0.3882353  0.69411767 0.85882354]\n",
            "   [0.30980393 0.5764706  0.77254903]\n",
            "   [0.34901962 0.5803922  0.7411765 ]]\n",
            "\n",
            "  [[0.22352941 0.7137255  0.91764706]\n",
            "   [0.17254902 0.72156864 0.98039216]\n",
            "   [0.19607843 0.7176471  0.9411765 ]\n",
            "   ...\n",
            "   [0.6117647  0.7137255  0.78431374]\n",
            "   [0.5529412  0.69411767 0.80784315]\n",
            "   [0.45490196 0.58431375 0.6862745 ]]\n",
            "\n",
            "  [[0.38431373 0.77254903 0.92941177]\n",
            "   [0.2509804  0.7411765  0.9882353 ]\n",
            "   [0.27058825 0.7529412  0.9607843 ]\n",
            "   ...\n",
            "   [0.7372549  0.7647059  0.80784315]\n",
            "   [0.46666667 0.5294118  0.5764706 ]\n",
            "   [0.23921569 0.30980393 0.3529412 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.28627452 0.30980393 0.3019608 ]\n",
            "   [0.20784314 0.24705882 0.26666668]\n",
            "   [0.21176471 0.26666668 0.3137255 ]\n",
            "   ...\n",
            "   [0.06666667 0.15686275 0.2509804 ]\n",
            "   [0.08235294 0.14117648 0.2       ]\n",
            "   [0.12941177 0.1882353  0.19215687]]\n",
            "\n",
            "  [[0.23921569 0.26666668 0.29411766]\n",
            "   [0.21568628 0.27450982 0.3372549 ]\n",
            "   [0.22352941 0.30980393 0.40392157]\n",
            "   ...\n",
            "   [0.09411765 0.1882353  0.28235295]\n",
            "   [0.06666667 0.13725491 0.20784314]\n",
            "   [0.02745098 0.09019608 0.1254902 ]]\n",
            "\n",
            "  [[0.17254902 0.21960784 0.28627452]\n",
            "   [0.18039216 0.25882354 0.34509805]\n",
            "   [0.19215687 0.3019608  0.4117647 ]\n",
            "   ...\n",
            "   [0.10588235 0.20392157 0.3019608 ]\n",
            "   [0.08235294 0.16862746 0.25882354]\n",
            "   [0.04705882 0.12156863 0.19607843]]]\n",
            "\n",
            "\n",
            " [[[0.7411765  0.827451   0.9411765 ]\n",
            "   [0.7294118  0.8156863  0.9254902 ]\n",
            "   [0.7254902  0.8117647  0.92156863]\n",
            "   ...\n",
            "   [0.6862745  0.7647059  0.8784314 ]\n",
            "   [0.6745098  0.7607843  0.87058824]\n",
            "   [0.6627451  0.7607843  0.8627451 ]]\n",
            "\n",
            "  [[0.7607843  0.8235294  0.9372549 ]\n",
            "   [0.7490196  0.8117647  0.9254902 ]\n",
            "   [0.74509805 0.80784315 0.92156863]\n",
            "   ...\n",
            "   [0.6784314  0.7529412  0.8627451 ]\n",
            "   [0.67058825 0.7490196  0.85490197]\n",
            "   [0.654902   0.74509805 0.84705883]]\n",
            "\n",
            "  [[0.8156863  0.85882354 0.95686275]\n",
            "   [0.8039216  0.84705883 0.9411765 ]\n",
            "   [0.8        0.84313726 0.9372549 ]\n",
            "   ...\n",
            "   [0.6862745  0.7490196  0.8509804 ]\n",
            "   [0.6745098  0.74509805 0.84705883]\n",
            "   [0.6627451  0.7490196  0.84313726]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.8117647  0.78039217 0.70980394]\n",
            "   [0.79607844 0.7647059  0.6862745 ]\n",
            "   [0.79607844 0.76862746 0.6784314 ]\n",
            "   ...\n",
            "   [0.5294118  0.5176471  0.49803922]\n",
            "   [0.63529414 0.61960787 0.5882353 ]\n",
            "   [0.65882355 0.6392157  0.5921569 ]]\n",
            "\n",
            "  [[0.7764706  0.74509805 0.6666667 ]\n",
            "   [0.7411765  0.70980394 0.62352943]\n",
            "   [0.7058824  0.6745098  0.5764706 ]\n",
            "   ...\n",
            "   [0.69803923 0.67058825 0.627451  ]\n",
            "   [0.6862745  0.6627451  0.6117647 ]\n",
            "   [0.6862745  0.6627451  0.6039216 ]]\n",
            "\n",
            "  [[0.7764706  0.7411765  0.6784314 ]\n",
            "   [0.7411765  0.70980394 0.63529414]\n",
            "   [0.69803923 0.6666667  0.58431375]\n",
            "   ...\n",
            "   [0.7647059  0.72156864 0.6627451 ]\n",
            "   [0.76862746 0.7411765  0.67058825]\n",
            "   [0.7647059  0.74509805 0.67058825]]]\n",
            "\n",
            "\n",
            " [[[0.8980392  0.8980392  0.9372549 ]\n",
            "   [0.9254902  0.92941177 0.96862745]\n",
            "   [0.91764706 0.9254902  0.96862745]\n",
            "   ...\n",
            "   [0.8509804  0.85882354 0.9137255 ]\n",
            "   [0.8666667  0.8745098  0.91764706]\n",
            "   [0.87058824 0.8745098  0.9137255 ]]\n",
            "\n",
            "  [[0.87058824 0.8666667  0.8980392 ]\n",
            "   [0.9372549  0.9372549  0.9764706 ]\n",
            "   [0.9137255  0.91764706 0.9647059 ]\n",
            "   ...\n",
            "   [0.8745098  0.8745098  0.9254902 ]\n",
            "   [0.8901961  0.89411765 0.93333334]\n",
            "   [0.8235294  0.827451   0.8627451 ]]\n",
            "\n",
            "  [[0.8352941  0.80784315 0.827451  ]\n",
            "   [0.91764706 0.9098039  0.9372549 ]\n",
            "   [0.90588236 0.9137255  0.95686275]\n",
            "   ...\n",
            "   [0.8627451  0.8627451  0.9098039 ]\n",
            "   [0.8627451  0.85882354 0.9098039 ]\n",
            "   [0.7921569  0.79607844 0.84313726]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.5882353  0.56078434 0.5294118 ]\n",
            "   [0.54901963 0.5294118  0.49803922]\n",
            "   [0.5176471  0.49803922 0.47058824]\n",
            "   ...\n",
            "   [0.8784314  0.87058824 0.85490197]\n",
            "   [0.9019608  0.89411765 0.88235295]\n",
            "   [0.94509804 0.94509804 0.93333334]]\n",
            "\n",
            "  [[0.5372549  0.5176471  0.49411765]\n",
            "   [0.50980395 0.49803922 0.47058824]\n",
            "   [0.49019608 0.4745098  0.4509804 ]\n",
            "   ...\n",
            "   [0.70980394 0.7058824  0.69803923]\n",
            "   [0.7921569  0.7882353  0.7764706 ]\n",
            "   [0.83137256 0.827451   0.8117647 ]]\n",
            "\n",
            "  [[0.47843137 0.46666667 0.44705883]\n",
            "   [0.4627451  0.45490196 0.43137255]\n",
            "   [0.47058824 0.45490196 0.43529412]\n",
            "   ...\n",
            "   [0.7019608  0.69411767 0.6784314 ]\n",
            "   [0.6431373  0.6431373  0.63529414]\n",
            "   [0.6392157  0.6392157  0.6313726 ]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggDYGPFnP3rb"
      },
      "source": [
        "The Numpy command to_categorical() is used to one-hot encode. This is why we imported the np_utils function from Keras, as it contains to_categorical().\n",
        "\n",
        "We also need to specify the number of classes that are in the dataset, so we know how many neurons to compress the final layer down to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AU4mn3WIaL1"
      },
      "source": [
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "class_num = y_test.shape[1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C4Tk8FGim-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b539308-2d13-41d2-8ecb-07249caff946"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRnX3Eo0QlRP"
      },
      "source": [
        "Designing the Model\n",
        " Keras has several different formats or blueprints to build models on, but Sequential is the most commonly used, and for that reason, we have imported it from Keras.\n",
        "\n",
        " The first layer of our model is a convolutional layer. It will take in the inputs and run convolutional filters on them.\n",
        "\n",
        "When implementing these in Keras, we have to specify the number of channels/filters we want (that's the 32 below), the size of the filter we want (3 x 3 in this case), the input shape (when creating the first layer) and the activation and padding we need.\n",
        "\n",
        "As mentioned, relu is the most common activation, and padding='same' just means we aren't changing the size of the image at all:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-S1CPtSI6xA"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# you can also string the activations and poolings together, like this:\n",
        "#model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBU6Po1Xj9Ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a54099-7edf-483e-fa3c-92faf17a1f46"
      },
      "source": [
        "print(X_train.shape[1:])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V9CZIipRubT"
      },
      "source": [
        "Now we will make a dropout layer to prevent overfitting, which functions by randomly eliminating some of the connections between the layers (0.2 means it drops 20% of the existing connections).\n",
        "\n",
        "We may also want to do batch normalization here. Batch Normalization normalizes the inputs heading into the next layer, ensuring that the network always creates activations with the same distribution that we desire.\n",
        "\n",
        "Then comes another convolutional layer, but the filter size increases so the network can learn more complex representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjesQqmOJLFw"
      },
      "source": [
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M07g4JsYScEP"
      },
      "source": [
        "Here's the pooling layer,  this helps make the image classifier more robust so it can learn relevant patterns. There's also the dropout and batch normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdwlhiPeJZl8"
      },
      "source": [
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytSUrc42TD0e"
      },
      "source": [
        "You can vary the exact number of convolutional layers you have to your liking, though each one adds more computation expenses. Notice that as you add convolutional layers you typically increase their number of filters so the model can learn more complex representations. If the numbers chosen for these layers seems somewhat arbitrary, just know that in general, you increase filters as you go on and it's advised to make them powers of 2 which can grant a slight benefit when training on a GPU.\n",
        "\n",
        "It's important not to have too many pooling layers, as each pooling discards some data. Pooling too often will lead to there being almost nothing for the densely connected layers to learn about when the data reaches them.\n",
        "\n",
        "The exact number of pooling layers you should use will vary depending on the task you are doing, and it's something you'll get a feel for over time. Since the images are so small here already we won't pool more than twice.\n",
        "\n",
        "You can now repeat these layers to give your network more representations to work off of:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk8FpihWJgUL"
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLwZyb_6TOh-"
      },
      "source": [
        "After we are done with the convolutional layers, we need to Flatten the data, which is why we imported the function above. We'll also add a layer of dropout again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuRu5l5wJo4J"
      },
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeXEx91fTRMA"
      },
      "source": [
        "Now we make use of the Dense import and create the first densely connected layer. We need to specify the number of neurons in the dense layer. Note that the numbers of neurons in succeeding layers decreases, eventually approaching the same number of neurons as there are classes in the dataset (in this case 10). The kernel constraint can regularize the data as it learns, another thing that helps prevent overfitting. This is why we imported maxnorm earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bslgsk4QJuHx"
      },
      "source": [
        "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ujgsc_T-UD"
      },
      "source": [
        "In this final layer, we pass in the number of classes for the number of neurons. Each neuron represents a class, and the output of this layer will be a 10 neuron vector with each neuron storing some probability that the image in question belongs to the class it represents.\n",
        "\n",
        "Finally, the softmax activation function selects the neuron with the highest probability as its output, voting that the image belongs to that class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdDpZzEQJ4sR"
      },
      "source": [
        "model.add(Dense(class_num))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7w9B27gUM6D"
      },
      "source": [
        "Let's specify the number of epochs we want to train for, as well as the optimizer we want to use.\n",
        "\n",
        "The optimizer is what will tune the weights in your network to approach the point of lowest loss. The Adam algorithm is one of the most commonly used optimizers because it gives great performance on most problems:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyBIcmA3J9n3"
      },
      "source": [
        "epochs = 25\n",
        "optimizer = 'adam'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3dxvyxBUf4y"
      },
      "source": [
        "Let's now compile the model with our chosen parameters. Let's also specify a metric to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1dWpi9jKF6j"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3vH2duaUifC"
      },
      "source": [
        "We can print out the model summary to see what the whole model looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6EfNH2uKNZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e1123c-1be1-4835-a110-43d90923a91f"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,264,458\n",
            "Trainable params: 2,263,114\n",
            "Non-trainable params: 1,344\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABuEc8PcU20e"
      },
      "source": [
        "Now we get to training the model. To do this, all we have to do is call the fit() function on the model and pass in the chosen parameters.\n",
        "\n",
        "Here's where I use the seed I chose, for the purposes of reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyNRgg_kKb0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f7b572b-63a6-412d-904a-29ba64b0b8b1"
      },
      "source": [
        "numpy.random.seed(seed)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "782/782 [==============================] - 47s 25ms/step - loss: 1.5110 - accuracy: 0.4665 - val_loss: 1.1733 - val_accuracy: 0.5762\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 1.0399 - accuracy: 0.6329 - val_loss: 0.8507 - val_accuracy: 0.7016\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.8640 - accuracy: 0.6943 - val_loss: 0.9540 - val_accuracy: 0.6688\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.7620 - accuracy: 0.7313 - val_loss: 0.7279 - val_accuracy: 0.7401\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.7030 - accuracy: 0.7542 - val_loss: 0.7072 - val_accuracy: 0.7533\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.6613 - accuracy: 0.7682 - val_loss: 0.7434 - val_accuracy: 0.7423\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.6315 - accuracy: 0.7777 - val_loss: 0.6531 - val_accuracy: 0.7735\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.6041 - accuracy: 0.7887 - val_loss: 0.6142 - val_accuracy: 0.7851\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.5740 - accuracy: 0.7987 - val_loss: 0.5669 - val_accuracy: 0.8040\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.5576 - accuracy: 0.8062 - val_loss: 0.7795 - val_accuracy: 0.7383\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.5409 - accuracy: 0.8112 - val_loss: 0.6045 - val_accuracy: 0.7959\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.5249 - accuracy: 0.8176 - val_loss: 0.5498 - val_accuracy: 0.8118\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.5034 - accuracy: 0.8237 - val_loss: 0.5412 - val_accuracy: 0.8145\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4923 - accuracy: 0.8273 - val_loss: 0.5762 - val_accuracy: 0.8041\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.4851 - accuracy: 0.8315 - val_loss: 0.5796 - val_accuracy: 0.8014\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4779 - accuracy: 0.8311 - val_loss: 0.5333 - val_accuracy: 0.8170\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4643 - accuracy: 0.8368 - val_loss: 0.5693 - val_accuracy: 0.8084\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4549 - accuracy: 0.8408 - val_loss: 0.5262 - val_accuracy: 0.8204\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4472 - accuracy: 0.8447 - val_loss: 0.5263 - val_accuracy: 0.8230\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4456 - accuracy: 0.8435 - val_loss: 0.5442 - val_accuracy: 0.8178\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4382 - accuracy: 0.8464 - val_loss: 0.5411 - val_accuracy: 0.8205\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4320 - accuracy: 0.8495 - val_loss: 0.5087 - val_accuracy: 0.8292\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4244 - accuracy: 0.8511 - val_loss: 0.6396 - val_accuracy: 0.7927\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4258 - accuracy: 0.8502 - val_loss: 0.5570 - val_accuracy: 0.8072\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4167 - accuracy: 0.8536 - val_loss: 0.5087 - val_accuracy: 0.8281\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9da0551950>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CklFCmPsKtcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e9f1d7-bff6-49de-adc3-29836383fc82"
      },
      "source": [
        "# Model evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 82.81%\n"
          ]
        }
      ]
    }
  ]
}